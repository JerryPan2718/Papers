# Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context

The Limitation of Transformer: 
- fixed-length context

Transformer-XL: 
- learning dependency beyond a fixed length without disrupting temporal coherence
- Segment-level recurrence mechanism + A Novel Positional Encoding Scheme

Results:
- Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation.