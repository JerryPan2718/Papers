# Transformer: Model Compression, Quantization, Pruning, Speedup

## Intro
- https://jalammar.github.io/illustrated-transformer/
- https://jalammar.github.io/illustrated-gpt2/

![13041641265430_ pic_hd](https://user-images.githubusercontent.com/37657480/148004166-3ad41193-c044-4007-99b9-c14565769a0d.jpg)
![13031641265348_ pic_hd](https://user-images.githubusercontent.com/37657480/148004148-2836d052-ce66-48a8-986e-97e514423df6.jpg)



## Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context
(https://github.com/kimiyoung/transformer-xl)

The Limitation of Transformer: 
- fixed-length context

Idea: (New Transformer Architecture)
- learning dependency beyond a fixed length without disrupting temporal coherence
- Segment-level recurrence mechanism + A Novel Positional Encoding Scheme

Results:
- Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation.

Code: (https://github.com/kimiyoung/transformer-xl/blob/master/pytorch/mem_transformer.py)
- cat = torch.cat([mems, w], 0) to reuse previous tokens to avoid recomputation.
- qkv_net() is a nn.Linear().


## Lite Transformer With Long-Short Range Attention (Zhanghao Wu)
(https://github.com/mit-han-lab/lite-transformer)
 
Idea: (Efficient Inference)
- Long-Short Range Attention(LSRA)
	- One group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention)
- Reduce the computation of the transformer to shrink the embedding size directly
	- The computation (Mult-Adds) is dominated by the feed-forward network (FFN)
- LSRA trades off the computation in FFN for wider attention layers. 
	- It stretches the bottleneck to introduce more dependency capturing capability for the attention layer, and then shrink the embedding size to reduce the total computation amount while maintaining the same performance
	- LSRA introduces convolution in a parallel branch to capture local dependencies so that the attention branch can focus on global context capture
- AutoML is not a panacea
	- Careful analysis and design insights (i.e., removing the bottleneck, specialized heads) can effectively prune the search space and improve the sample efficiency.

Results:
- Improved BLUE score under constrained resources(500M/100M MACs)
- Reduces the computation of transformer base model by 2.5X with 0.3 BLEU score degradation
- Combined with pruning and quantization, compressed the model size of Lite Transformer by 18.2X

## Pay Less Attention With Lightweight And Dynamic Convolutions
(https://github.com/pytorch/fairseq)

Idea: (New Transformer Architecture, Quantization)
- Lightweight convolutions
	- Depth-wise separable, softmax-normalized, and share weights over the channel dimension.
	- Different to self-attention, lightweight convolutions reuse the same weights for context elements, regardless of the current time-step.
- Dynamic convolutions build on lightweight convolutions by predicting a different convolution kernel at every time-step.
	- The kernel is a function of the current time-step only as opposed to the entire context as in self-attention.
	- The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic.
	- Dynamic convolutions are similar to locally connected layers in the sense that the weights change at every position, however, the difference is that weights are dynamically generated by the model rather than fixed after training

Results:
- Outperforms SOTA on WMT
	- Content-based self-attention is not necessary to achieve good accuracy on large translation benchmarks
- Speedup for sampling


## AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural Architecture Search

Idea: (Neural Architecture Search, Knowledge Distillation)
- AdaBERT
	- A novel compression method that leverages differentiable Neural Architecture Search to automatically compress BERT into task-adaptive small models for specific tasks.
- Task-oriented knowledge distillation loss 
	- Provides search hints and an efficiency-aware loss as search constraints, which enables a good trade-off between efficiency and effectiveness for task-adaptive BERT compression

Results:
- Smaller model while maintaining comparable performance


## MINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers
(https://github.com/microsoft/unilm/tree/master/minilm)

Idea: (Model Compression, Knowledge Distillation)
- Deep self-attention distillation
	- The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher).
	- Distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student.
- The scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works.
- A teacher assistant (Mirzadeh et al., 2019) also helps the distillation of large pre-trained Transformer models

Results:
- Outperforms SOTA in different parameter size of student models.

## DynaBERT: Dynamic BERT with Adaptive Width and Depth
(https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/DynaBERT)

Limitations of previous works:
- Recent works on BERT compression usually compress the large BERT model to a fixed smaller size. 
	- They can not fully satisfy the requirements of different edge devices with various hardware performances.

Idea: (Modle Compression, Knowledge Distillation)
- Dynamic BERT model (DynaBERT)
	- Flexibly adjusts the size and latency by selecting adaptive width and depth.
	- The training process of DynaBERT includes first training a width-adaptive BERT and then allowing both adaptive width and depth, by distilling knowledge from the full-sized model to small sub-networks
	- We distill knowledge from the full-sized teacher model to smaller student sub-networks to reduce the accuracy drop caused by the lower capacity of smaller size
	- Before allowing both adaptive width and depth, we train an only width-adaptive BERT (abbreviated as DynaBERTW) to act as a teacher assistant to bridge the large gap of model size between the student and teacher.
	- For DynaBERTW, we rewire the connections in each Transformer layer to ensure that the more important heads and neurons are utilized by more sub-networks.
	- Once DynaBERT is trained, no further fine-tuning is required for each sub-network

Results:
- Achieves comparable performances as BERTBASE (or RoBERTaBASE) with the same or smaller size
	- Ahe sub-network of DynaBERT or DynaRoBERTa with the maximum size does not necessarily have the best performance, indicating that redundancy exists in the original BERT or RoBERTa model
- The modelâ€™s width and depth for most tasks can be reduced without performance drop
- Compared to the depth direction, the width direction is more robust to compression.


## HAT: Hardware-Aware Transformers for Efficient Natural Language Processing
(https://github.com/mit-han-lab/hardware-aware-transformers)

Idea: (Neural Architecture Search)
- NAS
	- Constructs a large design space with arbitrary encoder-decoder attention and heterogeneous layers. 
	- Then we train a SuperTransformer that covers all candidates in the design space, and efficiently produces many SubTransformers with weight sharing. 
	- Finally, we perform an evolutionary search with a hardware latency constraint to find a specialized SubTransformer dedicated to run fast on the target hardware.

Results:
- Faster model inference
- GPU model is wide but shallow; the Raspberry Pi model is deep but thin. The phenomenon echos with our latency profiling.
	- It guides manual designs: on GPU, we can reduce the layer number and increase dimen- sion to reduce latency and keep high performance.
- Quantization Friedly + Knowledge Distillation Friendly


## Bringing TVM into TensorFlow for Optimizing Neural Machine Translation on GPU
(https://tvm.apache.org/2018/03/23/nmt-transformer-optimize)

Limitations:
- Batch matmul is a major performance hotspot in Transformer and the current implementation in cuBLAS is not well optimized.

Results:
- TVM generated kernel (with schedule optimization) brings at least 13X speedup for batch matmul computation, and a futher speedup with operator fusion enabled.
	- In Transformer, batch matmul is widely used in the computation of multi-head attention. Using batch matmul, multiple heads in the attention layer can run in parallel, which can help improve the computation efficiency of the hardware.


## TensorFlow Graph Optimizations
(https://web.stanford.edu/class/cs245/slides/TFGraphOptimizationsStanford.pdf)


# Computation Graph Optimization
## Loop-invariant Node Motion(LINM)
<img width="774" alt="Screen Shot 2021-12-31 at 12 50 43" src="https://user-images.githubusercontent.com/37657480/147838685-3714356c-ec03-4657-b3ba-705bd6c7f07c.png">

## MatMul replaced by Convolution
![WechatIMG1296](https://user-images.githubusercontent.com/37657480/147837538-c162c626-c28c-40ce-9646-5e2f771e6704.jpeg)

## QKV Combined Computation
<img width="1744" alt="Screen Shot 2021-12-31 at 11 51 46" src="https://user-images.githubusercontent.com/37657480/147837555-5a78e353-c252-46bc-ba72-e24d1ca26de3.png">

## FasterTransformer by NVIDIA
(https://github.com/NVIDIA/FasterTransformer)

